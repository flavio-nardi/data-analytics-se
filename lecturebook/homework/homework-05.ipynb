{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5\n",
    "\n",
    "## References\n",
    "\n",
    "+ Lectures 17-20 (inclusive).\n",
    "\n",
    "\n",
    "## Instructions\n",
    "\n",
    "+ Type your name and email in the \"Student details\" section below.\n",
    "+ Develop the code and generate the figures you need to solve the problems using this notebook.\n",
    "+ For the answers that require a mathematical proof or derivation you should type them using latex. If you have never written latex before and you find it exceedingly difficult, we will likely accept handwritten solutions.\n",
    "+ The total homework points are 100. Please note that the problems are not weighed equally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(precision=3)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set(rc={\"figure.dpi\":100, \"savefig.dpi\":300})\n",
    "sns.set_context(\"notebook\")\n",
    "sns.set_style(\"ticks\")\n",
    "\n",
    "import scipy\n",
    "import scipy.stats as st\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "def download(\n",
    "    url : str,\n",
    "    local_filename : str = None\n",
    "):\n",
    "    \"\"\"Download a file from a url.\n",
    "    \n",
    "    Arguments\n",
    "    url            -- The url we want to download.\n",
    "    local_filename -- The filemame to write on. If not\n",
    "                      specified \n",
    "    \"\"\"\n",
    "    if local_filename is None:\n",
    "        local_filename = os.path.basename(url)\n",
    "    urllib.request.urlretrieve(url, local_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student details\n",
    "\n",
    "+ **First Name: Flavio**\n",
    "+ **Last Name: Nardi**\n",
    "+ **Email: fnardi@purdue.edu**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1 - Clustering Uber Pickup Data\n",
    "\n",
    "In this problem you will analyze Uber pickup data collected during April 2014 around New York City.\n",
    "The complete data are freely on [Kaggle](https://www.kaggle.com/fivethirtyeight/uber-pickups-in-new-york-city/).\n",
    "The data consist of a timestamp (which we are going to ignore), the latitude and longitude of the Uber pickup, and a base code (which we are also ignoring).\n",
    "The data file we are going to use is [uber-raw-data-apr14.csv](https://raw.githubusercontent.com/PredictiveScienceLab/data-analytics-se/master/homework/uber-raw-data-apr14.csv).\n",
    "As usual, you have to make it visible to this Jupyter notebook.\n",
    "On Google Colab, just run this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://github.com/PredictiveScienceLab/data-analytics-se/raw/master/lecturebook/data/uber-raw-data-apr14.csv\"\n",
    "download(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And you can load it using pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "p1_data = pd.read_csv('uber-raw-data-apr14.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a text view:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see, there where about half a million Uber pickups during April 2014...\n",
    "Let's extract the lattitude and longitude data only (this is needed for passing them to scikit-learn algorithms).\n",
    "Here is how you can do this in pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just use the column names as indices.\n",
    "# The two brackets are required because you are actually\n",
    "# passing a list of columns\n",
    "loc_data = p1_data[['Lon', 'Lat']]\n",
    "loc_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also visualize these points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(loc_data.Lon, loc_data.Lat, s=0.01)\n",
    "# ``s=0.01`` specifies the size. I am using a small size because\n",
    "# these are too many points to visualize\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is nice, but it would be even nicer if we had a map of New York City on the background.\n",
    "We can make such a map on [www.openstreetmap.org](https://www.openstreetmap.org/export#map=11/40.7855/-73.8964).\n",
    "We just need to have a box of longitude's and latitudes that overlaps with our data.\n",
    "Here is how to get such a *bounding box*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box = ((loc_data.Lon.min(), loc_data.Lon.max(),\n",
    "        loc_data.Lat.min(), loc_data.Lat.max()))\n",
    "box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have already extracted this picture for you and you can find it [here](https://github.com/PredictiveScienceLab/data-analytics-se/blob/master/homework/ny_map.png).\n",
    "As always, it needs to be visible from the Jupyter notebook.\n",
    "On Google Colab run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://github.com/PredictiveScienceLab/data-analytics-se/raw/master/lecturebook/images/ny_map.png\"\n",
    "download(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have it at the right place, you should be able to see the image here:\n",
    "\n",
    "![New York City Map](ny_map.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load the image as a matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ny_map = plt.imread('ny_map.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can visualize it with ``plt.imshow`` and draw the Uber pickups on top of it.\n",
    "Here is how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=600)\n",
    "ax.scatter(\n",
    "    loc_data.Lon,\n",
    "    loc_data.Lat,\n",
    "    zorder=1,\n",
    "    alpha= 0.5,\n",
    "    c='b',\n",
    "    s=0.001\n",
    ")\n",
    "ax.set_xlim(box[0],box[1])\n",
    "ax.set_ylim(box[2],box[3])\n",
    "ax.imshow(\n",
    "    ny_map,\n",
    "    zorder=0,\n",
    "    extent=box,\n",
    "    aspect= 'equal'\n",
    ")\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we have over half a million data points, machine learning algorithms may be a bit slow.\n",
    "So, as you develop your code use use only 50K observations.\n",
    "Once you have a stable version of your code, modify the following code segment to make use of the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# While your are developing your code use this:\n",
    "#p1_train_data = loc_data[:100000]\n",
    "# When you have a stable code, use this:\n",
    "p1_train_data = loc_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A - Splitting New York City into Subregions\n",
    "\n",
    "Suppose that you are assigned the task of splitting New York City into operating subregions with pretty much equal demand.\n",
    "When a pickup is requested in each subregion only the drivers in that region are called.\n",
    "Note that this can quickly become a very difficult problem very quickly.\n",
    "We are not looking for the best possible answer here.\n",
    "This would require posing and solving a suitable optimization problem.\n",
    "We are looking for a data-informed solution that is compatible with common sense.\n",
    "\n",
    "Do (at least) the following:\n",
    "+ Use Kmeans clustering on the pickup data with different number of clusters;\n",
    "+ Visualize the labels of the clusters on the map using different colors (see the hands-on activities);\n",
    "+ Visualize the centers of the discovered Kmeans clusters (in red color);\n",
    "+ Use your common sense, e.g., make sure that you have enough clusters so that no region crosses the water (even if it is possible the drivers may have to pay tolls to cross). If it is impossible to get perfect results simply by Kmeans, feel free to ignore a small number of outliers as they could be handled manually;\n",
    "+ Use [MiniBatchKMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.MiniBatchKMeans.html#sklearn.cluster.MiniBatchKMeans) which is an much faster version of Kmeans suitable for large datasets (>10K observations);\n",
    "\n",
    "Answer with as many text blocks and code blocks as you like right below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "model_uber_data = MiniBatchKMeans(n_clusters=32,\n",
    "                                  random_state=0,\n",
    "                                  batch_size=1024,\n",
    "                                  max_iter=10).fit(p1_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = model_uber_data.predict(p1_train_data)\n",
    "fig, ax = plt.subplots(dpi=600)\n",
    "ax.scatter(\n",
    "    p1_train_data.Lon,\n",
    "    p1_train_data.Lat,\n",
    "    zorder=1,\n",
    "    alpha= 0.5,\n",
    "    c=labels,\n",
    "    s=0.001\n",
    ")\n",
    "ax.scatter(\n",
    "    model_uber_data.cluster_centers_[:, 0],\n",
    "    model_uber_data.cluster_centers_[:, 1],\n",
    "    c='r',\n",
    "    zorder=2,\n",
    "    s=0.5)\n",
    "ax.set_xlim(box[0],box[1])\n",
    "ax.set_ylim(box[2],box[3])\n",
    "ax.imshow(\n",
    "    ny_map,\n",
    "    zorder=0,\n",
    "    extent=box,\n",
    "    aspect= 'equal'\n",
    ")\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B - Create a Stochastic Model of Pickups\n",
    "\n",
    "One of the key ingredients for a more sophisticated approach to optimizing the operations of Uber would involve the construction of a stochastic model of the demand for pickups.\n",
    "The ideal model for this problem is the [Poisson Point Process](https://en.wikipedia.org/wiki/Poisson_point_process).\n",
    "However, we are going to do something simpler, using the Gaussian mixture model and a Poisson random variable.\n",
    "The model will not have a time component, but it will allow us to sample the number and locations of pickups during a typical month.\n",
    "We will guide you through the process of constructing this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subpart B.I - Random variable capturing number of monthly pickups\n",
    "\n",
    "Find the rate of monthly pickups (ignore the fact that months may differ by a few days) and use it to define a Poisson random variable corresponding to the monthly number of pickups.\n",
    "Use ``scipy.stats.poisson`` to initialize this random variable. Sample from it 10,000 times and plot the histogram of the samples to get a feeling about the corresponding probability mass function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "monthly_pickup_rate = p1_data.shape[0]\n",
    "monthly_pickup_poisson = scipy.stats.poisson(monthly_pickup_rate)\n",
    "samples = monthly_pickup_poisson.rvs(10000)\n",
    "\n",
    "fix, ax = fig, ax = plt.subplots()\n",
    "ax.hist(samples, density = False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subpart B.II - Estimate the spatial density of pickups\n",
    "\n",
    "Fit a Gaussian Mixture model to the pickup data.\n",
    "**Do not use the Bayesian Information Criterion** to decide how many components to keep.\n",
    "This would take quite a bit of time for this problem. Simply use 40 mixture components.\n",
    "Plot the contour of the logarithm of the probability density on the New York City map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "spatial_density_model = GaussianMixture(n_components=40).fit(p1_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make grid\n",
    "x = np.linspace(box[0], box[1])\n",
    "y = np.linspace(box[2], box[3])\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "# Get PDF on grid points\n",
    "XX = np.array([X.ravel(), Y.ravel()]).T\n",
    "z = spatial_density_model.score_samples(XX)\n",
    "Z = z.reshape(X.shape)\n",
    "\n",
    "fig, ax = plt.subplots(dpi=600)\n",
    "c = ax.contour(\n",
    "    X,\n",
    "    Y,\n",
    "    Z,\n",
    "    levels=np.linspace(-100, 5.0, 40)\n",
    ")\n",
    "plt.colorbar(c)\n",
    "ax.set_xlim(box[0],box[1])\n",
    "ax.set_ylim(box[2],box[3])\n",
    "ax.imshow(\n",
    "    ny_map,\n",
    "    zorder=0,\n",
    "    extent=box,\n",
    "    aspect= 'equal'\n",
    ")\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subpart B.III - Sample some random months of pickups\n",
    "\n",
    "Now that you have a model that gives you the number of pickups and a model that allows you to sample a pickup location, sample five different datasets (number of pickups and location of each pick) from the combined model and visualize them on the New York map.\n",
    "\n",
    "**Hint:** Don't get obsessed with making the model perfect. It's okay if a few of the pickups are on water..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_samples(\n",
    "    data,\n",
    "    labels,\n",
    "):\n",
    "    \"\"\"Plot NY map and samples.\n",
    "    \n",
    "    Argumets\n",
    "    data  -- The samples.\n",
    "    labels -- The labels.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(dpi=600)\n",
    "    ax.scatter(\n",
    "        sim_data[:, 0],\n",
    "        sim_data[:, 1],\n",
    "        zorder=1,\n",
    "        alpha= 0.5,\n",
    "        c=sim_labels,\n",
    "        s=0.001\n",
    "    )\n",
    "    ax.set_xlim(box[0],box[1])\n",
    "    ax.set_ylim(box[2],box[3])\n",
    "    ax.imshow(\n",
    "        ny_map,\n",
    "        zorder=0,\n",
    "        extent=box,\n",
    "        aspect= 'equal'\n",
    "    )\n",
    "    ax.set_xlabel('Longitude')\n",
    "    ax.set_ylabel('Latitude');\n",
    "\n",
    "for i in range(5):\n",
    "    monthly_pickup_number = monthly_pickup_poisson.rvs()\n",
    "    sim_data, sim_labels = spatial_density_model.sample(monthly_pickup_number)\n",
    "    plot_samples(sim_data, sim_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2 - Counting Celestial Objects\n",
    "\n",
    "Consider [this picture](https://github.com/PredictiveScienceLab/data-analytics-se/raw/master/datasets/galaxies.png) of a patch of sky taken by the [Hubble Space Telescope](https://www.nasa.gov/mission_pages/hubble/story/index.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/PredictiveScienceLab/data-analytics-se/master/lecturebook/images/galaxies.png'\n",
    "download(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![galaxies](galaxies.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This picture includes many galaxies, but also some starts.\n",
    "We are going to create a machine learning model capable of counting the number of objects in such images.\n",
    "Our model is not going to be able to differentiate between the different types of objects, and it will not be very accurate, but it does form the basis of more sophisticated approaches.\n",
    "The idea is as follows:\n",
    "+ Convert the picture to points sampled according to the intensity of light.\n",
    "+ Apply Gaussian mixture on the resulting points.\n",
    "+ Use the Bayesian Information Criterion to identify the number of components in the picture.\n",
    "+ Associate the number of components with the actual number of celestial objects.\n",
    "\n",
    "I will set you up with the first step. You will have to do the last three."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to load the image with the [Python Imaging Library (PIL)](https://en.wikipedia.org/wiki/Python_Imaging_Library) which allows us to apply a few basic transformations to the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "hubble_image = Image.open('galaxies.png')\n",
    "# here is how to see the image\n",
    "hubble_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to convert it to gray scale and crop it to make the problem a little bit easier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = hubble_image.convert('L').crop((100, 100, 300, 300))\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that black-and white images are matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_ar = np.array(img)\n",
    "img_ar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The minimum number if $0$ corresponding to black and the maximum number is 255 corresponding to white.\n",
    "Anything in between is some shade of gray."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, imagine that each pixel is associated with some cordinates.\n",
    "Without loss of generality, let's assume that each pixel is some coordinate in $[0,1]^2$.\n",
    "We will loop over each of the pixels and sample its coordinates in a way that increases with increasing light intensity.\n",
    "To achieve this, we will pass the intensity values of each pixels through a sigmoid with parameters that can be tuned.\n",
    "Here is this sigmoid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intensities = np.linspace(0, 255, 255)\n",
    "fig, ax = plt.subplots()\n",
    "alpha = 0.1\n",
    "beta = 255 / 3\n",
    "ax.plot(\n",
    "    intensities,\n",
    "    1.0 / (1.0 + np.exp(-alpha * (intensities - beta)))\n",
    ");\n",
    "ax.set_xlabel('Light intensities')\n",
    "ax.set_ylabel('Probability of sampling the pixel coordinates');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is the code that samples the pixel coordinates.\n",
    "I am organizing it into a function because we may want to use it with different pictures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_pixel_coords(img, alpha, beta):\n",
    "    \"\"\"\n",
    "    Samples pixel coordinates based on a probability defined as the sigmoid of the intensity.\n",
    "    \n",
    "    Arguments:\n",
    "    \n",
    "        img    -     The gray scale pixture from which we sample as an array\n",
    "        alpha     -     The scale of the sigmoid\n",
    "        beta      -     The offset of the sigmoid\n",
    "    \"\"\"\n",
    "    img_ar = np.array(img)\n",
    "    x = np.linspace(0, 1, img_ar.shape[0])\n",
    "    y = np.linspace(0, 1, img_ar.shape[1])\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    img_to_locs = []\n",
    "    # Loop over pixels\n",
    "    for i in range(img_ar.shape[1]):\n",
    "        for j in range(img_ar.shape[0]):\n",
    "            # Calculate the probability of the pixel by looking at each\n",
    "            # light intensity\n",
    "            prob = 1.0 / (1.0 + np.exp(-alpha * (img_ar[j, i] - beta)))\n",
    "            # Pick a uniform random number\n",
    "            u = np.random.rand()\n",
    "            # If u is smaller than the desired probability,\n",
    "            # the consider the coordinates of the pixel sampled\n",
    "            if u <= prob:\n",
    "                img_to_locs.append((Y[i, j], X[-i-1, -j-1]))\n",
    "    # Turn img_to_locs into a numpy array\n",
    "    img_to_locs = np.array(img_to_locs)\n",
    "    return img_to_locs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locs = sample_pixel_coords(img, alpha=0.1, beta=200)\n",
    "fig, ax = plt.subplots(dpi=150)\n",
    "ax.imshow(img, extent=((0, 1, 0, 1)), zorder=0)\n",
    "ax.scatter(\n",
    "    locs[:, 0],\n",
    "    locs[:, 1],\n",
    "    zorder=1,\n",
    "    alpha=0.5,\n",
    "    c='b',\n",
    "    s=1\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that by playing with $\\alpha$ and $\\beta$ you can make the whole thing more or less sensitive to the light intensity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "def count_objs(img, alpha, beta, nc_min=1, nc_max=50):\n",
    "    \"\"\"Count objects in image.\n",
    "    \n",
    "    Arguments:\n",
    "        img       -     The image\n",
    "        alpha     -     The scale of the sigmoid\n",
    "        beta      -     The offset of the sigmoid\n",
    "        nc_min    -     The minimum number of components to consider\n",
    "        nc_max    -     The maximum number of components to consider\n",
    "    \"\"\"\n",
    "    locs = sample_pixel_coords(img, alpha, beta)\n",
    "    # **** YOUR CODE HERE ****\n",
    "    # Use BIC to search for the best GaussianMixture model\n",
    "    # with components between nc_min and nc_max\n",
    "    bics = np.ndarray((nc_max - 1,))\n",
    "    models = []\n",
    "    for nc in range(1, nc_max):\n",
    "        m = GaussianMixture(n_components=nc).fit(locs)\n",
    "        bics[nc - 1] = m.bic(locs)\n",
    "        models.append(m)\n",
    "    # Set the following variables\n",
    "    idx_best_model = np.argmin(bics)\n",
    "    best_nc = idx_best_model\n",
    "    best_model = models[idx_best_model]\n",
    "\n",
    "    return best_nc, best_model, locs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have completed the code, try it out the following images.\n",
    "Feel free to play with $\\alpha$ and $\\beta$ to improve the performance.\n",
    "**Do not try to make a perfect model. To do so, we would have to go beyond the Gaussian mixture model. This is just a homework problem.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a helper function for visualizing what you get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_counts(img, objs, model, locs):\n",
    "    \"\"\"Visualize the counts.\n",
    "    \n",
    "    Arguments\n",
    "    img    --  The image.\n",
    "    objs   --  Returned by count_objs()\n",
    "    model  --  Returned by count_objs()\n",
    "    locs   --  Returned by count_objs()\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(dpi=150)\n",
    "    ax.imshow(img, extent=((0, 1, 0, 1)))\n",
    "    for i in range(model.means_.shape[0]):\n",
    "        ax.plot(\n",
    "            model.means_[i, 0],\n",
    "            model.means_[i, 1],\n",
    "            'wx', \n",
    "            markersize=(\n",
    "                10.0 * model.weights_.shape[0]\n",
    "                * model.weights_[i]\n",
    "            )\n",
    "        )\n",
    "    ax.scatter(\n",
    "        locs[:, 0],\n",
    "        locs[:, 1],\n",
    "        zorder=1,\n",
    "        alpha=0.5,\n",
    "        c='b',\n",
    "        s=1\n",
    "    )\n",
    "    ax.set_title('Counted {0:d} objects!'.format(objs));  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how to use the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objs, model, locs = count_objs(img, alpha=0.1, beta=150)\n",
    "visualize_counts(img, objs, model, locs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try this one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = hubble_image.convert('L').crop((200, 200, 400, 400))\n",
    "objs, model, locs = count_objs(img, alpha=.1, beta=250)\n",
    "visualize_counts(img, objs, model, locs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And try this one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = hubble_image.convert('L').crop((300, 300, 500, 500))\n",
    "objs, model, locs = count_objs(img, alpha=.1, beta=250)\n",
    "visualize_counts(img, objs, model, locs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3 - Filtering of an Oscillator with Damping\n",
    "\n",
    "Assume that you are dealing with a one-degree-of-freedom system which follows the equation:\n",
    "\n",
    "$$\n",
    "\\ddot{x} + 2\\zeta \\omega_0 \\dot{x} + \\omega^2_0 x = u_0 \\cos(\\omega t),\n",
    "$$\n",
    "\n",
    "where $x = x(t)$ is the generalized coordinate of the oscillator at time $t$, and the parameters $\\zeta$, $\\omega_0$, $u_0$, and $\\omega$ are known to you (we will give them specific values later).\n",
    "Furthermore, assume that you are making noisy observations of the *absolute acceleration* at discrete timesteps $\\Delta t$ (also known):\n",
    "\n",
    "$$\n",
    "y_j = \\ddot{x}(j\\Delta t)-u_0 \\cos(\\omega t)+w_j,\n",
    "$$\n",
    "\n",
    "for $j=1,\\dots,n$, where $w_j \\sim N(0, \\sigma^2)$ with $\\sigma^2$ also known.\n",
    "Finally, assume that the initial conditions for the position and the velocity (you need both to get a unique solution) are given by:\n",
    "\n",
    "$$\n",
    "x_0 = x(0) \\sim N(0, \\sigma_x^2),\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "v_0 = \\dot{x} \\sim N(0, \\sigma_v^2).\n",
    "$$\n",
    "\n",
    "Of course assume taht $\\sigma_x^2$ and $\\sigma_v^2$ are specific numbers that we are going to specify below.\n",
    "\n",
    "Before I we go over the questions, let's write code that generates the true trajectory of the system at some random initial conditions as well as some observations.\n",
    "We will use the code to generate a synthetic dataset with known ground truth which you will use in your filtering analysis.\n",
    "\n",
    "The first step we need to do, is to turn the problem into a first order differential equation.\n",
    "This is trivial.\n",
    "We set:\n",
    "\n",
    "$$\n",
    "\\mathbf{x} = \n",
    "\\begin{bmatrix}\n",
    "x\\\\\n",
    "\\dot{x}\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "Assuming $\\mathbf{x} = (x_1,x_2)$, then the dynamics are described by:\n",
    "\n",
    "$$\n",
    "\\dot{\\mathbf{x}} = \n",
    "\\begin{bmatrix}\n",
    "\\dot{x}\\\\\n",
    "\\ddot{x}\n",
    "\\end{bmatrix}\n",
    "= \n",
    "\\begin{bmatrix}\n",
    "x_2\\\\\n",
    "-2\\zeta \\omega_0 \\dot{x} - \\omega^2_0 x + u_0 \\cos(\\omega t)\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "x_2\\\\\n",
    "-2\\zeta \\omega_0 x_2 - \\omega^2_0 x_1 + u_0 \\cos(\\omega t)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The initial conditions are of course just:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_0 =\n",
    "\\begin{bmatrix}\n",
    "x_0\\\\\n",
    "v_0\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "This first order system can solved using [scipy.integrate.solve_ivp](https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.solve_ivp.html#scipy.integrate.solve_ivp).\n",
    "Here is how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.integrate import solve_ivp\n",
    "\n",
    "# You need to define the right hand side of the equation\n",
    "def rhs(t, x, omega0, zeta, u0, omega):\n",
    "    \"\"\"Return the right hand side of the dynamical system.\n",
    "    \n",
    "    Arguments\n",
    "    t       -    Time\n",
    "    x       -    The state\n",
    "    omega0  -    Natural frequency\n",
    "    zeta    -    Dumping factor (0<=zeta)\n",
    "    u0      -    External force amplitude\n",
    "    omega   -    Excitation frequency\n",
    "    \"\"\"\n",
    "    res = np.ndarray((2,))\n",
    "    res[0] = x[1]\n",
    "    res[1] = -2.0 * zeta * omega0 * x[1] - omega0 ** 2 * x[0] + u0 * np.cos(omega * t)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is how you solve it for given initial conditions and parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial conditions\n",
    "x0 = np.array([0.0, 1.0])\n",
    "# Natural frequency\n",
    "omega0 = 2.0\n",
    "# Dumping factor\n",
    "zeta = 0.4\n",
    "# External forcing amplitude\n",
    "u0 = 0.5\n",
    "# Excitation frequency\n",
    "omega = 2.1\n",
    "# Timestep\n",
    "dt = 0.1\n",
    "# The final time\n",
    "final_time = 10.0\n",
    "# The number of timesteps to get the final time\n",
    "n_steps = int(final_time / dt)\n",
    "# The times on which you want the solution\n",
    "t_eval = np.linspace(0, final_time, n_steps)\n",
    "# The solution\n",
    "sol = solve_ivp(rhs, (0, final_time), x0, t_eval=t_eval, args=(omega0, zeta, u0, omega))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution is stored here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol.y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see that the shape is ``number of states x number of time steps``.\n",
    "Let's visualize the trajectory separating position and velocity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=150)\n",
    "ax.plot(t_eval, sol.y[0, :], label='Position')\n",
    "ax.plot(t_eval, sol.y[1, :], label='Velocity')\n",
    "ax.set_xlabel('$t$')\n",
    "ax.set_ylabel('$x_i(t)$')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now generate some synthetic observations of the acceleration with some given Gaussian noise.\n",
    "To get the true acceleration you can do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_acc = np.array([rhs(t, x, omega0, zeta, u0, omega)[1] for (t, x) in zip(t_eval, sol.y.T)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now I am going to add some Gaussian noise to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The measurement standard deviation\n",
    "sigma_r = 0.2\n",
    "observations = true_acc + sigma_r * np.random.randn(true_acc.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is how the noisy observations of the acceleration look like compared to the true acceleration value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=150)\n",
    "ax.plot(t_eval, true_acc, label='Acceleration')\n",
    "ax.plot(\n",
    "    t_eval,\n",
    "    observations,\n",
    "    '.',\n",
    "    label='Noisy observation of acceleration'\n",
    ")\n",
    "ax.set_xlabel('$t$')\n",
    "ax.set_ylabel('$\\ddot{x}(t)$')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay. Now imagine that you only see the noisy observations of the acceleration.\n",
    "The filtering goal is to recover the state of the underlying system (as well as its acceleration).\n",
    "I am going to guide you through the steps you need to follow.\n",
    "\n",
    "## Part A - Discretize time (Transitions)\n",
    "\n",
    "Use the Euler time discretization scheme to turn the continuous dynamical system into a discrete time dynamical system like this:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_{j+1} = \\mathbf{A}\\mathbf{x}_j + Bu_j + \\mathbf{z}_j,\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_j = \\mathbf{x}(j\\Delta t),\n",
    "$$\n",
    "\n",
    "$$\n",
    "u_j = u(j\\Delta t),\n",
    "$$\n",
    "\n",
    "and $\\mathbf{z}_j$ is properly chosen process noise term.\n",
    "You should derive and provide mathematical exprssions for the following:\n",
    "+ The $2 \\times 2$ transition matrix $\\mathbf{A}$.\n",
    "+ The $2 \\times 1$ control \"matrix\" $B$.\n",
    "+ The process covariance $\\mathbf{Q}$. For the process covariance, you may choose your own values by hand.\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Assuming $\\mathbf{x} = (x_1,x_2)$, then the dynamics are described by:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\dot{x}_1\\\\\n",
    "\\dot{x}_2\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "x_2\\\\\n",
    "-2\\zeta \\omega_0 x_2 - \\omega^2_0 x_1 + u_0 \\cos(\\omega t)\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "0 & 1\\\\\n",
    "- \\omega^2_0 & -2\\zeta \\omega_0\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "{x}_1\\\\\n",
    "{x}_2\n",
    "\\end{bmatrix}\n",
    "+ \\begin{bmatrix}\n",
    "0 \\\\\n",
    "1\n",
    "\\end{bmatrix}\n",
    "u_0 \\cos(\\omega t)\n",
    "$$\n",
    "Let's discretize\n",
    "$$\n",
    "\\dot{x}_i\n",
    "\\approx\n",
    "\\frac{x_{i,k+1} - x_{i,k}}{\\Delta t}\n",
    "$$\n",
    "then\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "x_{1,k+1}\\\\\n",
    "x_{2,k+1}\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "x_{1,k}\\\\\n",
    "x_{2,k}\n",
    "\\end{bmatrix} + \n",
    "\\begin{bmatrix}\n",
    "0 & \\Delta t\\\\\n",
    "- \\omega^2_0 \\Delta t & -2\\zeta \\omega_0 \\Delta t\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_{1,k}\\\\\n",
    "x_{2,k}\n",
    "\\end{bmatrix}\n",
    "+ \\begin{bmatrix}\n",
    "0 \\\\\n",
    "\\Delta t\n",
    "\\end{bmatrix}\n",
    "u_0 \\cos(\\omega t)\n",
    "$$\n",
    "and simplifying\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "x_{1,k+1}\\\\\n",
    "x_{2,k+1}\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "1 & \\Delta t\\\\\n",
    "- \\omega^2_0 \\Delta t & 1-2\\zeta \\omega_0 \\Delta t\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_{1,k}\\\\\n",
    "x_{2,k}\n",
    "\\end{bmatrix}\n",
    "+ \\begin{bmatrix}\n",
    "0 \\\\\n",
    "\\Delta t\n",
    "\\end{bmatrix}\n",
    "u_0 \\cos(\\omega t)\n",
    "$$\n",
    "We can conclude that\n",
    "$$\n",
    "A = \n",
    "\\begin{bmatrix}\n",
    "1 & \\Delta t\\\\\n",
    "- \\omega^2_0 \\Delta t & 1-2\\zeta \\omega_0 \\Delta t\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "B =\n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "\\Delta t\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You should be using the parameters dt, omega0, zeta, etc.\n",
    "# from above\n",
    "A = np.array(\n",
    "    [\n",
    "        [1.0, dt],\n",
    "        [-omega0 ** 2 * dt, 1.0-2 * zeta * omega0 * dt]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = np.array(\n",
    "    [\n",
    "        [0.0],\n",
    "        [dt]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = np.array(\n",
    "    [\n",
    "        [0.001, 0.0],\n",
    "        [0.0, 0.001]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B - Discretize time (Emissions)\n",
    "\n",
    "Establish the map that takes you from the states to the accelerations at each timestep.\n",
    "That is, specify:\n",
    "\n",
    "$$\n",
    "y_j = \\mathbf{C}\\mathbf{x}_j + w_j,\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "y_j = \\ddot{x}(j\\Delta t)-u_0 \\cos(\\omega t)+w_j,\n",
    "$$\n",
    "\n",
    "and $w_j$ is a measurement noise.\n",
    "You should derive and provide mathematical expressions for the following:\n",
    "+ The $1 \\times 2$ emission matrix $\\mathbf{C}$.\n",
    "+ The $1 \\times 1$ covariance \"matrix\" $R$ of the measurement noise.\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "$$\n",
    "y_{j} = \\ddot{x}(j\\Delta t)-u_0 \\cos(\\omega t)+w_j\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\ddot{x}(j\\Delta t) = \n",
    "\\dot{x}_2(j\\Delta t) =\n",
    "\\begin{bmatrix}\n",
    "- \\omega^2_0 \\Delta t & 1-2\\zeta \\omega_0 \\Delta t\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_{1,j}\\\\\n",
    "x_{2,j}\n",
    "\\end{bmatrix} +\n",
    "u_0 \\cos(\\omega t).\n",
    "$$\n",
    "Plugging in the last expression in the previous one:\n",
    "$$\n",
    "y_{j} = \n",
    "\\begin{bmatrix}\n",
    "- \\omega^2_0 \\Delta t & 1-2\\zeta \\omega_0 \\Delta t\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_{1,j}\\\\\n",
    "x_{2,j}\n",
    "\\end{bmatrix}\n",
    "+\n",
    "w_j.\n",
    "$$\n",
    "Hence\n",
    "$$\n",
    "C = \n",
    "\\begin{bmatrix}\n",
    "- \\omega^2_0 \\Delta t & 1-2\\zeta \\omega_0 \\Delta t\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "R\n",
    "= \n",
    "\\sigma_w\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.array(\n",
    "    [\n",
    "        [-omega0 ** 2, 1.0 -2 * zeta * omega0]\n",
    "    ]\n",
    ")\n",
    "\n",
    "R = np.array(\n",
    "    [\n",
    "        [sigma_r ** 2]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C - Apply the Kalman filter\n",
    "\n",
    "Use ``FilterPy`` (see the hands-on activity of Lecture 20) to infer the unobserved states given the noisy observations of the accelerations.\n",
    "Plot time-evolving 95% credible intervals for the position and the velocity along with the true unobserved values of these quantities (in two separate plots)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here (as many code and text blocks as you want)\n",
    "from filterpy.kalman import KalmanFilter\n",
    "kf = KalmanFilter(dim_x=2, dim_z=1)\n",
    "kf.x = np.zeros((2,1))\n",
    "kf.P = np.array([1**2, 1**2]) * np.eye(2)\n",
    "kf.Q = Q\n",
    "kf.R = R\n",
    "kf.H = C\n",
    "kf.F = A\n",
    "kf.B = B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = dt * np.arange(n_steps + 1)\n",
    "us = np.zeros(n_steps)\n",
    "us[:] = u0 * np.cos(omega0 * times[1:])\n",
    "# means, covs, _, _ = kf.batch_filter(observations, us)\n",
    "\n",
    "means = np.zeros((n_steps, 2))\n",
    "covs = np.zeros((n_steps, 2, 2))\n",
    "output = np.zeros((n_steps, 1))\n",
    "for n in range(1, n_steps):\n",
    "    # Predict step\n",
    "    kf.predict(u=us[n])\n",
    "    # Measurement update step\n",
    "    kf.update(observations[n])\n",
    "    means[n, :] = kf.x[:, 0]\n",
    "    covs[n, :, :] = kf.P\n",
    "    output[n] = np.dot(kf.H, kf.x[:, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_kf_estimates(means, covs):\n",
    "    \"\"\"Plot estimates of the state with 95% credible intervals.\"\"\"\n",
    "    y_labels = ['$x_1$', '$x_2$']\n",
    "\n",
    "    dpi = 150\n",
    "    \n",
    "    res_x = 1024\n",
    "    res_y = 768\n",
    "\n",
    "    w_in = res_x / dpi\n",
    "    h_in = res_y / dpi\n",
    "\n",
    "    fig, ax = plt.subplots(2, 1)\n",
    "    fig.set_size_inches(w_in, h_in)\n",
    "\n",
    "    times = dt * np.arange(n_steps + 1)\n",
    "\n",
    "    for j in range(2):\n",
    "        ax[j].set_ylabel(y_labels[j])\n",
    "    ax[-1].set_xlabel('$t$ (time)')\n",
    "\n",
    "    for j in range(2):\n",
    "        ax[j].plot(\n",
    "            times[0:n_steps],\n",
    "            sol.y[j, :],\n",
    "            'b.-'\n",
    "        )\n",
    "        ax[j].plot(\n",
    "            times[1:n_steps],\n",
    "            means[1:n_steps, j],\n",
    "            'r.'\n",
    "        )\n",
    "        ax[j].fill_between(\n",
    "            times[1:n_steps],\n",
    "            (\n",
    "                means[1:n_steps, j] - 2.0 * np.sqrt(covs[1:n_steps, j, j])\n",
    "            ),\n",
    "            (\n",
    "                means[1:n_steps, j] + 2.0 * np.sqrt(covs[1:n_steps, j, j])\n",
    "            ),\n",
    "            color='red',\n",
    "            alpha=0.25\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_kf_estimates(means, covs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(times[0:n_steps], output)\n",
    "plt.plot(times[0:n_steps], observations)\n",
    "plt.plot(times[0:n_steps], true_acc)\n",
    "\n",
    "mean_y = np.dot(C, means.T)\n",
    "plt.plot(times[0:n_steps], mean_y.T);\n",
    "\n",
    "mean_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part D - Quantify and visualize your uncertainty about the true acceleration value\n",
    "\n",
    "Use standard uncertainty propagation techniques to quantify your epistemic uncertainty about the true acceleration value.\n",
    "You will have to use the inferred states of the system and the dynamical model.\n",
    "This can be done either analytically or by Monte Carlo. It's your choice.\n",
    "In any case, plot time-evolving 95% credible intervals for the acceleration (epistemic only) along with the true unobserved values and the noisy measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "samples = np.zeros((N, means.shape[0], 1))\n",
    "for j in range(N):\n",
    "    for i in range(1, means.shape[0]):\n",
    "        states = st.multivariate_normal(mean=means[i, :], cov=covs[i, :, :])\n",
    "        samples[j, i, :] = np.dot(C, states.rvs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_bound = np.mean(samples, axis=0) - 2.0 * np.std(samples, axis=0)\n",
    "upper_bound = np.mean(samples, axis=0) + 2.0 * np.std(samples, axis=0)\n",
    "\n",
    "mean_y = np.mean(samples, axis=0)\n",
    "plt.plot(times[0:n_steps], true_acc)\n",
    "plt.fill_between(\n",
    "    times[0:n_steps],\n",
    "    (\n",
    "        lower_bound[:, 0]\n",
    "    ),\n",
    "    (\n",
    "        upper_bound[:, 0]\n",
    "    ),\n",
    "    color='b',\n",
    "    alpha=0.25\n",
    ")\n",
    "plt.plot(times[0:n_steps], mean_y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
